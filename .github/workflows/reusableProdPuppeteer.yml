name: reusableProdPuppeteer

on: 
  workflow_call:
    inputs:
      site:
        description: 'Site name to crawl'
        required: true
        type: string

env:
  site: ${{inputs.site}}
  MONGODB_URL: ${{secrets.MONGODB_URL}}
  GH_TOKEN: ${{secrets.GH_TOKEN}}
  GOOGLE_SERVICE_ACCOUNT_CREDENTIALS: ${{secrets.GOOGLE_SERVICE_ACCOUNT_CREDENTIALS}}
  GOOGLE_SHEET_ID: ${{secrets.GOOGLE_SHEET_ID}}
  GOOGLE_SHEET_ID_FOR_LOGS: ${{secrets.GOOGLE_SHEET_ID_FOR_LOGS}}
  GOOGLE_DRIVE_FOLDER_ID: ${{secrets.GOOGLE_DRIVE_FOLDER_ID}}
  GET_LOCAL_SITE_CONF: 'TRUE'

jobs:
  crawler-job:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js with caching
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
        cache-dependency-path: '**/package-lock.json'

    - name: Download sheet data artifact
      id: download-artifact
      uses: actions/download-artifact@v4
      with:
        name: sheet-data
        path: .
      continue-on-error: true

    - name: Setup cached sheet data
      run: |
        CACHE_STATUS="none"
        
        if [ -f sheet-data.json ]; then
          # Validate JSON structure
          if jq empty sheet-data.json 2>/dev/null; then
            cp sheet-data.json siteConfig.json
            echo "✅ Found and validated cached sheet data"
            echo "📊 Sheet data info:"
            echo "  Timestamp: $(cat siteConfig.json | jq -r '.timestamp // "Unknown"')"
            echo "  Rows: $(cat siteConfig.json | jq '.data | length' 2>/dev/null || echo 'Unknown')"
            echo "  Source: Artifact cache"
            CACHE_STATUS="artifact"
          else
            echo "⚠️  Found sheet-data.json but it's invalid JSON"
            rm -f sheet-data.json siteConfig.json
            CACHE_STATUS="invalid"
          fi
        else
          echo "ℹ️  No cached data artifact found"
          
          # Check if there's already a siteConfig.json (from previous run or manual setup)
          if [ -f siteConfig.json ]; then
            if jq empty siteConfig.json 2>/dev/null; then
              echo "✅ Found existing siteConfig.json"
              echo "  Source: Pre-existing file"
              CACHE_STATUS="existing"
            else
              echo "⚠️  Found siteConfig.json but it's invalid"
              rm -f siteConfig.json
              CACHE_STATUS="invalid"
            fi
          else
            CACHE_STATUS="none"
          fi
        fi
        
        case $CACHE_STATUS in
          "artifact"|"existing")
            echo "🚀 Will use cached configuration data"
            ;;
          "none"|"invalid")
            echo "🌐 Will fetch fresh data from Google Sheets API"
            ;;
        esac
        
        echo "CACHE_STATUS=$CACHE_STATUS" >> $GITHUB_ENV

    - name: Cache Puppeteer browsers
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/puppeteer
          ~/.local/share/puppeteer
          node_modules/puppeteer/.local-chromium
        key: ${{ runner.os }}-puppeteer-${{ hashFiles('**/package-lock.json') }}
        restore-keys: |
          ${{ runner.os }}-puppeteer-

    - name: Install dependencies
      run: |
        npm ci --prefer-offline --no-audit

    - name: Run crawler
      run: |
        echo "🚀 Starting crawler for site: ${{ inputs.site }}"
        echo "📊 Cache status: $CACHE_STATUS"
        
        if [ "$CACHE_STATUS" = "artifact" ] || [ "$CACHE_STATUS" = "existing" ]; then
          echo "📁 Using cached Google Sheets data"
        else
          echo "🌐 Will fetch fresh data from Google Sheets API"
        fi
        
        node scripts/crawl-ecommerce-site.js

    - name: Upload results
      if: always()
      run: |
        echo "📤 Uploading results..."
        node node src/2_data/persistence/sheet/uploadToGoogleSheet.js