name: Kadin Canta Aggregator
on:
  workflow_dispatch:
    inputs:
      site_limit:
        description: 'Limit the number of sites to run for testing (optional)'
        required: false
        type: number
        default: 0 # 0 means no limit
      site:
        description: 'Run for a single site (e.g., "brand-name"). Overrides schedule/site_limit.'
        required: false
        type: string
  schedule:
    - cron: '0 5 * * 1'

env:
  MONGODB_URL: ${{secrets.MONGODB_URL}}
  GH_TOKEN: ${{secrets.GH_TOKEN}}
  GOOGLE_SERVICE_ACCOUNT_CREDENTIALS: ${{secrets.GOOGLE_SERVICE_ACCOUNT_CREDENTIALS}}
  GOOGLE_SHEET_ID: ${{secrets.GOOGLE_SHEET_ID}}
  GOOGLE_SHEET_ID_FOR_LOGS: ${{secrets.GOOGLE_SHEET_ID_FOR_LOGS}}

jobs:
  prepare_matrix:
    runs-on: ubuntu-latest
    outputs:
      sites: ${{ steps.set-matrix.outputs.sites }}
      has-sites: ${{ steps.set-matrix.outputs.has-sites }}
      has-sheet-data: ${{ steps.set-matrix.outputs.has-sheet-data }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      
      - name: Install dependencies
        run: npm install googleapis
      
      - name: Create package.json for CommonJS
        run: |
          cat > package.json << 'EOF'
          {
            "type": "commonjs"
          }
          EOF
      
      - name: Fetch sheet data and prepare matrix
        id: set-matrix
        env:
          SITE_LIMIT: ${{ github.event.inputs.site_limit }}
          SINGLE_SITE: ${{ github.event.inputs.site }}
        run: |
          cat > fetch_data.js << 'EOF'
          const { google } = require('googleapis');
          const fs = require('fs');

          async function fetchAllSheetData() {
            try {
              console.log('ðŸ” Starting Google Sheets data fetch...');

              if (!process.env.GOOGLE_SERVICE_ACCOUNT_CREDENTIALS) throw new Error('GOOGLE_SERVICE_ACCOUNT_CREDENTIALS not set');
              if (!process.env.GOOGLE_SHEET_ID) throw new Error('GOOGLE_SHEET_ID not set');

              const decodedCredentials = Buffer.from(process.env.GOOGLE_SERVICE_ACCOUNT_CREDENTIALS, 'base64').toString('utf8');
              const credentials = JSON.parse(decodedCredentials);

              const auth = new google.auth.GoogleAuth({
                credentials,
                scopes: ['https://www.googleapis.com/auth/spreadsheets.readonly'],
              });

              const sheets = google.sheets({ version: 'v4', auth });
              const response = await sheets.spreadsheets.values.get({
                spreadsheetId: process.env.GOOGLE_SHEET_ID,
                range: 'wbags-scroll!A:K',
              });

              const rows = response.data.values;
              const sheetData = { timestamp: new Date().toISOString(), runId: process.env.GITHUB_RUN_ID || 'local', data: rows };
              fs.writeFileSync('sheet-data.json', JSON.stringify(sheetData, null, 2));

              const singleSite = process.env.SINGLE_SITE;
              if (singleSite && singleSite.trim()) {
                console.log(`ðŸƒ Running for single site: ${singleSite}`);
                return { sites: [singleSite.trim()] };
              }

              const dataRows = rows.slice(1);
              const sites = [];
              const siteLimit = process.env.SITE_LIMIT ? parseInt(process.env.SITE_LIMIT, 10) : 0;
              let sitesAdded = 0;

              for (let i = 0; i < dataRows.length; i++) {
                if (siteLimit > 0 && sitesAdded >= siteLimit) break;

                const row = dataRows[i];
                const brandName = row[0];
                const isPaused = row[8];

                if (brandName && brandName.trim() && brandName.toLowerCase() !== 'brands' &&
                    isPaused !== 'TRUE' && isPaused !== 'true' && brandName.trim().length > 1) {
                  sites.push(brandName.trim());
                  sitesAdded++;
                }
              }

              return { sites };
            } catch (error) {
              console.error(error);
              process.exit(1);
            }
          }

          fetchAllSheetData().then(result => {
            const sitesJson = JSON.stringify(result.sites);
            const hasSites = result.sites.length > 0 ? 'true' : 'false';
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `sites=${sitesJson}\n`);
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `has-sheet-data=true\n`);
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `has-sites=${hasSites}\n`);
          }).catch(error => {
            console.error(error);
            process.exit(1);
          });
          EOF

          node fetch_data.js

      - name: Upload sheet data artifact
        uses: actions/upload-artifact@v4
        if: steps.set-matrix.outputs.has-sheet-data == 'true' # This will now always be true
        with:
          name: sheet-data
          path: sheet-data.json
          retention-days: 1

  dizitv_job:
    needs: prepare_matrix
    if: needs.prepare_matrix.outputs.has-sites == 'true'
    strategy:
      fail-fast: false
      max-parallel: 5
      matrix:
        site: ${{ fromJSON(needs.prepare_matrix.outputs.sites) }}
    uses: ./.github/workflows/reusableProdPuppeteer.yml
    with:
      site: ${{ matrix.site }}
    secrets: inherit

  summarize_run:
    runs-on: ubuntu-latest
    needs: dizitv_job # This job runs only after all matrix jobs are finished
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Download all run artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/ # Download all artifacts into an 'artifacts' directory

      - name: List downloaded files
        run: ls -R artifacts/

      - name: Generate Final Summary Report
        id: summary-report
        run: |
          # The script needs a directory with summary files, which are inside subdirectories.
          # We'll find all upload-summary.json files and pass their parent directory to the script.
          # The script is designed to read from the directory passed as an argument.
          # The download-artifact action creates a directory for each artifact.
          node scripts/summarize-run.js artifacts/

      - name: Upload Final Summary Report
        uses: actions/upload-artifact@v4
        with:
          name: final-run-summary
          path: artifacts/final-summary.json

      - name: Upload Aggregated Metrics to Google Sheet
        if: always()
        run: |
          npm ci --prefer-offline --no-audit
          node scripts/upload-summary-to-sheet.js artifacts/final-summary.json
